{
  "id": "9e7b4479-590e-43ee-91fb-2492f7ce3c6f",
  "document_id": "f415f288-b390-46cb-81f1-fce4ab28ed7f",
  "content": "Attention Is All You Need Ashish Vaswani\u2217 Google Brain avaswani@google.com Noam Shazeer\u2217 Google Brain noam@google.com Niki Parmar\u2217 Google Research nikip@google.com Jakob Uszkoreit\u2217 Google Research usz@google.com Llion Jones\u2217 Google Research llion@google.com Aidan N. Gomez\u2217\u2020 University of Toronto aidan@cs.toronto.edu \u0141ukasz Kaiser\u2217 Google Brain lukaszkaiser@google.com Illia Polosukhin\u2217\u2021 illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions...",
  "version": 1,
  "is_current": true,
  "created_at": "2025-04-21T12:24:11.906554"
}